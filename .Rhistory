names.arg = levels(training$ASEX), ylim = c(0,0.60))
percentage <- paste(apply(prop.table(table(training$ASEX)), 1, function(x) {round(x*100,2)}), "%")
text(x = sex, y = prop.table(table(training$ASEX)),
label = percentage,
pos = 3, cex = 1, col = "red")
sex <- barplot(prop.table(table(training$ASEX)), main = "Sex of the population",
names.arg = levels(training$ASEX), ylim = c(0,0.60),
col = c("pink","blue"))
sex <- barplot(prop.table(table(training$ASEX)), main = "Sex of the population",
names.arg = levels(training$ASEX), ylim = c(0,0.60),
col = c("pink","caderblue3"))
sex <- barplot(prop.table(table(training$ASEX)), main = "Sex of the population",
names.arg = levels(training$ASEX), ylim = c(0,0.60),
col = c("pink","cadetblue3"))
sex <- barplot(prop.table(table(training$ASEX)), main = "Sex of the population",
names.arg = levels(training$ASEX), ylim = c(0,0.60),
col = c("sienna2","cadetblue3"))
percentage <- paste(apply(prop.table(table(training$ASEX)), 1, function(x) {round(x*100,2)}), "%")
text(x = sex, y = prop.table(table(training$ASEX)),
label = percentage,
pos = 3, cex = 1)
pie(table(training$AMARITL), main="Variable AMARITL",
labels=levels(training$AMARITL),col=rainbow(length(levels(training$AMARITL))))
pie(table(training$ARACE), main="Variable v11",
labels=levels(training$ARACE),col=rainbow(length(levels(training$ARACE))))
pie(table(training$ARACE), main="Variable ARACE",
labels=levels(training$ARACE),col=topo.colors(length(levels(training$ARACE))))
pie(table(training$ARACE), main="Variable ARACE",
labels=levels(training$ARACE),col=heat.colors(length(levels(training$ARACE))))
pie(table(training$ARACE), main="Variable ARACE",
labels=levels(training$ARACE),col=terrain.colors(length(levels(training$ARACE))))
pie(table(training$ARACE), main="Variable ARACE",
labels=levels(training$ARACE),col=topo.colors(length(levels(training$ARACE))))
pie(table(training$AMARITL), main="Variable AMARITL",
labels=levels(training$AMARITL),col=topo.colors(length(levels(training$AMARITL))))
pie(table(training$AMARITL), main="Variable AMARITL",
labels=levels(training$AMARITL),col=rainbow.colors(length(levels(training$AMARITL))))
pie(table(training$AMARITL), main="Variable AMARITL",
labels=levels(training$AMARITL),col=rainbow(length(levels(training$AMARITL))))
barplot(table(training$ACLSWKR),names.arg=levels(training$ACLSWKR))
barplot(table(training$ACLSWKR),names.arg=levels(training$ACLSWKR),las=2)
barplot(table(training$ACLSWKR),
names.arg=sapply(levels(training$ACLSWKR),FUN = function(x) {paste(strwrap(x, width = 16), collapse = "<br>")}),las=2)
barplot(table(training$ACLSWKR),
names.arg=sapply(levels(training$ACLSWKR),FUN = function(x) {paste(strwrap(x, width = 10), collapse = "<br>")}),las=2)
barplot(table(training$ACLSWKR),
names.arg=sapply(levels(training$ACLSWKR),FUN = function(x) {paste(strwrap(x, width = 10), collapse = "\n")}),las=2)
barplot(table(training$ACLSWKR),
names.arg=sapply(levels(training$ACLSWKR),FUN = function(x) {paste(strwrap(x, width = 8), collapse = "\n")}),las=2)
barplot(table(training$ACLSWKR), cex.axis = 0.8,
names.arg=sapply(levels(training$ACLSWKR),FUN = function(x) {paste(strwrap(x, width = 8), collapse = "\n")}),las=2)
x
barplot(table(training$ACLSWKR), cex.xaxis = 0.8,
names.arg=sapply(levels(training$ACLSWKR),FUN = function(x) {paste(strwrap(x, width = 8), collapse = "\n")}),las=2)
barplot(table(training$ACLSWKR), cex.axis = 0.6,
names.arg=sapply(levels(training$ACLSWKR),FUN = function(x) {paste(strwrap(x, width = 8), collapse = "\n")}),las=2)
barplot(table(training$ACLSWKR), cex.names = 0.6,
names.arg=sapply(levels(training$ACLSWKR),FUN = function(x) {paste(strwrap(x, width = 8), collapse = "\n")}),las=2)
barplot(table(training$ACLSWKR), cex.names = 0.6, cex.axis = 0.7
names.arg=sapply(levels(training$ACLSWKR),FUN = function(x) {paste(strwrap(x, width = 8), collapse = "\n")}),las=2)
barplot(table(training$ACLSWKR), cex.names = 0.6, cex.axis = 0.7,
names.arg=sapply(levels(training$ACLSWKR),FUN = function(x) {paste(strwrap(x, width = 8), collapse = "\n")}),las=2)
barplot(table(training$ACLSWKR), cex.names = 0.5, cex.axis = 0.7,
names.arg=sapply(levels(training$ACLSWKR),FUN = function(x) {paste(strwrap(x, width = 8), collapse = "\n")}),las=2)
barplot(table(training$ACLSWKR), cex.names = 0.7, cex.axis = 0.7,
names.arg=sapply(levels(training$ACLSWKR),FUN = function(x) {paste(strwrap(x, width = 8), collapse = "\n")}),las=2)
barplot(table(training$ACLSWKR), cex.names = 0.8, cex.axis = 0.7,
names.arg=sapply(levels(training$ACLSWKR),FUN = function(x) {paste(strwrap(x, width = 8), collapse = "\n")}),las=2)
barplot(table(training$ACLSWKR), cex.names = 0.8, cex.axis = 0.8,
main = "Variable AMJIND",
names.arg=sapply(levels(training$ACLSWKR),FUN = function(x) {paste(strwrap(x, width = 8), collapse = "\n")}),las=2)
plot(training$ARACE)
plot(table(training$ACLSWKR), cex.names = 0.8, cex.axis = 0.8,
main = "Variable AMJIND",
names.arg=sapply(levels(training$ACLSWKR),FUN = function(x) {paste(strwrap(x, width = 8), collapse = "\n")}),las=2)
r
barplot(table(training$ACLSWKR), cex.names = 0.8, cex.axis = 0.8,
main = "Variable AMJIND",
names.arg=sapply(levels(training$ACLSWKR),FUN = function(x) {paste(strwrap(x, width = 8), collapse = "\n")}),las=2)
hist( training$AAGE, col = grey(0.9), border = grey(0.2),
main = paste("Variable AAGE"),
xlab = "Age [year]",
ylab = "Population", las = 3, xlim = c(0,100), ylim = c(0, 40000),
labels = T,
breaks = seq(from = 0, to = 100, length = 11))
plot(training$AHSCOL, cex.names = 0.8, cex.axis = 0.8,
main = "Variable AHSCOL")
summary(training$AHSCOL)
plot(training$AHGA, cex.names = 0.8, cex.axis = 0.8,
main = "Variable AHGA")
pie(table(training$AHGA), cex.names = 0.8, cex.axis = 0.8,
main = "Variable AHGA")
pie(table(training$AHGA), cex.labels = 0.6, cex.axis = 0.8,
main = "Variable AHGA")
pie(table(training$AHGA), cex.names = 0.6,
main = "Variable AHGA")
pie(table(training$AHGA), cex = 0.6,
main = "Variable AHGA")
pie(table(training$AHGA), cex = 0.55,
main = "Variable AHGA")
pie(table(training$AHGA), cex = 0.55,
main = "Variable AHGA", col = rainbow(length(levels(training$AHGA)))
)
pie(table(training$AHGA), cex = 0.5,
main = "Variable AHGA", col = rainbow(length(levels(training$AHGA)))
)
plot.factor(training$AMJIND)
install.packages("graphics")
plot.factor(training$AMJIND)
plot(training$AMJIND)
plot(training$AMJIND,las=2)
plot(training$AMJIND,las=2, cex = 0.5)
plot(training$AMJIND,las=2, cex.axis = 0.5)
plot(training$AMJIND,las=2, cex.nales = 0.5)
plot(training$AMJIND,las=2, cex.names = 0.5)
library(glmnet)
library(caret)
#logistic <- glmnet(x=as.matrix(testFrame2), y=z, family="binomial", alpha=1)
#predict(logistic, as.matrix(testFrame2))
setwd("C:/Users/Bénédicte/Documents/Dataiku")
training <- read.csv("Data/census_income_learn.csv", header=F)
colnames(training) <- c("AAGE","ACLSWKR","ADTIND","ADTOCC", "AHGA",
"AHRSPAY", "AHSCOL", "AMARITL", "AMJIND",
"AMJOCC", "ARACE", "AREORGN", "ASEX", "AUNMEM",
"AUNTYPE", "AWKSTAT", "CAPGAIN", "CAPLOSS", "DIVVAL",
"FILESTAT", "GRINREG", "GRINST", "HHDFMX",
"HHDREL", "MARSUPWT", "MIGMTR1", "MIGMTR3", "MIGMTR4",
"MIGSAME", "MIGSUN","NOEMP", "PARENT",
"PEFNTVTY","PEMNTVTY","PENATVTY","PRCITSHP",
"SEOTR", "VETQVA", "VETYN", "WKSWORK","YEAR", "INCOME")
training <- training[sample(nrow(training)),]
z <- training[,"INCOME"]
training <- subset(training, select=-c(MARSUPWT))
training <- subset(training, select=-c(GRINST,HHDFMX,INCOME))
training <- data.modificationOfCountryOfBirth(training)
training <- data.modificationBinaryVariable(training)
dmy <- dummyVars(" ~ .", data = training)
training <- data.frame(predict(dmy, newdata = training))
training <- training[, !colnames(training) %in% colnames(training)[which(grepl( "Not.in.universe", colnames(training)))]]
logistic <- cv.glmnet(x=as.matrix(training), y=z, family="binomial", alpha=1, nfolds=5)
coeffs(logistic)
coef(logistic, s= "lambda.min")
test <- read.csv("Data/census_income_test.csv", header=F)
colnames(test) <- c("AAGE","ACLSWKR","ADTIND","ADTOCC", "AHGA",
"AHRSPAY", "AHSCOL", "AMARITL", "AMJIND",
"AMJOCC", "ARACE", "AREORGN", "ASEX", "AUNMEM",
"AUNTYPE", "AWKSTAT", "CAPGAIN", "CAPLOSS", "DIVVAL",
"FILESTAT", "GRINREG", "GRINST", "HHDFMX",
"HHDREL", "MARSUPWT", "MIGMTR1", "MIGMTR3", "MIGMTR4",
"MIGSAME", "MIGSUN","NOEMP", "PARENT",
"PEFNTVTY","PEMNTVTY","PENATVTY","PRCITSHP",
"SEOTR", "VETQVA", "VETYN", "WKSWORK","YEAR", "INCOME")
ztst <- test[,"INCOME"]
test <- subset(test, select=-c(MARSUPWT))
test <- subset(test, select=-c(GRINST,HHDFMX,INCOME))
test <- data.modificationOfCountryOfBirth(test)
test <- data.modificationBinaryVariable(test)
dmy <- dummyVars(" ~ .", data = test)
test <- data.frame(predict(dmy, newdata = test))
test <- test[, !colnames(test) %in% colnames(test)[which(grepl( "Not.in.universe", colnames(test)))]]
predtst <- as.factor(predict(logistic,
as.matrix(test),
s = "lambda.min",
type = "class"))
error <- sum(predtst != ztst)/length(ztst)
error
predtst
mc <- matrix(0, nrow=2,ncol=2)
mc[1,1] <- sum(predtst[which(ztst==1)] == ztst[which(ztst==1)])
mc[1,2] <- sum(predtst[which(ztst==1)] != ztst[which(ztst==1)])
mc[2,1] <- sum(predtst[which(ztst==2)] != ztst[which(ztst==2)])
mc[2,2] <- sum(predtst[which(ztst==2)] == ztst[which(ztst==2)])
mc
ztst
ztst[1] == -5000
ztst[1] == -50000
ztst[1]
ztst[1] == -50000.
predtst <- as.numeric(predtsts)
ztsts <- as.numeric(ztst)
ztst <- as.numeric(ztst)
mc <- matrix(0, nrow=2,ncol=2)
mc[1,1] <- sum(predtst[which(ztst==1)] == ztst[which(ztst==1)])
mc[1,2] <- sum(predtst[which(ztst==1)] != ztst[which(ztst==1)])
mc[2,1] <- sum(predtst[which(ztst==2)] != ztst[which(ztst==2)])
mc[2,2] <- sum(predtst[which(ztst==2)] == ztst[which(ztst==2)])
mc
predtsts
predtst
predtst <- as.numeric(predtst)
ztst <- as.numeric(ztst)
mc <- matrix(0, nrow=2,ncol=2)
mc[1,1] <- sum(predtst[which(ztst==1)] == ztst[which(ztst==1)])
mc[1,2] <- sum(predtst[which(ztst==1)] != ztst[which(ztst==1)])
mc[2,2] <- sum(predtst[which(ztst==2)] == ztst[which(ztst==2)])
mc[2,1] <- sum(predtst[which(ztst==2)] != ztst[which(ztst==2)])
mc
coef(logistic, as.matrix(test), s= "lambda.min")
order(coef(logistic, as.matrix(test), s= "lambda.min"))
as.matrix(coef(logistic, as.matrix(test), s= "lambda.min"))
coef <- as.matrix(coef(logistic, as.matrix(test), s= "lambda.min"))
coef <- as.matrix(coef(logistic, s= "lambda.min"))
View(coef)
order(coef)
training <- scale(training)
View(training)
logistic <- cv.glmnet(x=as.matrix(training), y=z, family="binomial", alpha=1, nfolds=5)
scale_center <- matrix(attributes(training)$"scaled:center",nrow=1,ncol=ncol(test))
scale_scale <- matrix(attributes(training)$"scaled:scale", nrow=1,ncol=ncol(test))
test <- scale(test, center=scale_center, scale=scale_scale)
predtst <- as.factor(predict(logistic,
as.matrix(test),
s = "lambda.min",
type = "class"))
error <- sum(predtst != ztst)/length(ztst)
error
predtst <- as.numeric(predtst)
error <- sum(predtst != ztst)/length(ztst)
error
mc <- matrix(0, nrow=2,ncol=2)
mc[1,1] <- sum(predtst[which(ztst==1)] == ztst[which(ztst==1)])
mc[1,2] <- sum(predtst[which(ztst==1)] != ztst[which(ztst==1)])
mc[2,1] <- sum(predtst[which(ztst==2)] != ztst[which(ztst==2)])
mc[2,2] <- sum(predtst[which(ztst==2)] == ztst[which(ztst==2)])
mc
coef <- as.matrix(coef(logistic, s= "lambda.min"))
View(coef)
sort(coef)
sum(coef)
coef <- abs(coef)
sum(coef)
logistic
boxplot(training$AAGE~z, col=c("red","green"), xlab="Diabète (1 si diabétique, 2 sinon)", ylab="nombre de grossesses")
boxplot(as.data.frame(training)$AAGE~z, col=c("red","green"), xlab="Diabète (1 si diabétique, 2 sinon)", ylab="nombre de grossesses")
boxplot(training$AAGE~z, col=c("sienna2","cadetblue3"), xlab="Income", ylab="nombre de grossesses")
boxplot(as.data.frame(training)$AAGE~z, col=c("sienna2","cadetblue3"), xlab="Income", ylab="nombre de grossesses")
setwd("C:/Users/Bénédicte/Documents/Dataiku")
training <- read.csv("Data/census_income_learn.csv", header=F)
colnames(training) <- c("AAGE","ACLSWKR","ADTIND","ADTOCC", "AHGA",
"AHRSPAY", "AHSCOL", "AMARITL", "AMJIND",
"AMJOCC", "ARACE", "AREORGN", "ASEX", "AUNMEM",
"AUNTYPE", "AWKSTAT", "CAPGAIN", "CAPLOSS", "DIVVAL",
"FILESTAT", "GRINREG", "GRINST", "HHDFMX",
"HHDREL", "MARSUPWT", "MIGMTR1", "MIGMTR3", "MIGMTR4",
"MIGSAME", "MIGSUN","NOEMP", "PARENT",
"PEFNTVTY","PEMNTVTY","PENATVTY","PRCITSHP",
"SEOTR", "VETQVA", "VETYN", "WKSWORK","YEAR", "INCOME")
z <- training[,"INCOME"]
boxplot(training$AAGE~z, col=c("sienna2","cadetblue3"), xlab="Income", ylab="Age")
boxplot(training$ADTOCC~z, col=c("sienna2","cadetblue3"), xlab="Income", ylab="Age")
boxplot(training$ASEX~z, col=c("sienna2","cadetblue3"), xlab="Income", ylab="Age")
boxplot(training$ASEX~z, col=c("sienna2","cadetblue3"), xlab="Income", ylab="Sex")
boxplot(training$AAGE~z, col=c("sienna2","cadetblue3"), xlab="Income", ylab="Age")
boxplot(training$ASEX~z, col=c("sienna2","cadetblue3"), xlab="Income", ylab="Sex")
table(training$ASEX,z)
table(training$ADTOCC,z)
boxplot(training$WKSWORK~z, col=c("sienna2","cadetblue3"), xlab="Income", ylab="Sex")
summary(training$WKSWORK~z)
summary(training[which(as.numeric(z)==1)
summary(training[which(as.numeric(z)==1)
)
summary(training[which(as.numeric(z)==1))
summary(training[which(as.numeric(z)==1)])
summary(training[,which(as.numeric(z)==1)])
summary(training$WKSWORK[which(as.numeric(z)==1)])
summary(training$WKSWORK[which(as.numeric(z)==2)])
tcode <- table(training$ADTOCC,z)
tcode <- apply(tcode,2, function(x) {x-sum(x)})
tcode
tcode <- table(training$ADTOCC,z)
tcode <- apply(tcode,2, function(x) {x/sum(x)})
tcode
tcode <- table(training$ADTOCC,z)
tcode <- apply(tcode,2, function(x) {(x-sum(x))*100})
tcode
tcode
tcode <- table(training$ADTOCC,z)
tcode <- apply(tcode,2, function(x) {x/sum(x)*100})
tcode
tcode <- table(training$AMJOCC,z)
tcode
tcode <- apply(tcode,2, function(x) {x/sum(x)*100})
tcode
summary(logistic)
logistic
plot(logistic, xvar="lambda")
predtst <- as.factor(predict(logistic,
as.matrix(test),
s = "lambda.1se",
type = "class"))
error <- sum(predtst != ztst)/length(ztst)
error
predtst <- as.numeric(predtst)
error <- sum(predtst != ztst)/length(ztst)
error
plot(fit10, main="LASSO")
install.packages("pROC")
library("pROC", lib.loc="~/R/win-library/3.3")
roc(ztst,predtst)
roc(ztst,predtst,plot=TRUE)
summary(tr)
prob <- predict(tr, testFrame2)
plot(pr)
plot(tr)
cv.tree(tr)
setwd("C:/Users/Bénédicte/Documents/Dataiku")
source("Scripts/data.R")
source("Scripts/separ1.R")
library(tree)
training <- read.csv("Data/census_income_learn.csv", header=F)
colnames(training) <- c("AAGE","ACLSWKR","ADTIND","ADTOCC", "AHGA",
"AHRSPAY", "AHSCOL", "AMARITL", "AMJIND",
"AMJOCC", "ARACE", "AREORGN", "ASEX", "AUNMEM",
"AUNTYPE", "AWKSTAT", "CAPGAIN", "CAPLOSS", "DIVVAL",
"FILESTAT", "GRINREG", "GRINST", "HHDFMX",
"HHDREL", "MARSUPWT", "MIGMTR1", "MIGMTR3", "MIGMTR4",
"MIGSAME", "MIGSUN","NOEMP", "PARENT",
"PEFNTVTY","PEMNTVTY","PENATVTY","PRCITSHP",
"SEOTR", "VETQVA", "VETYN", "WKSWORK","YEAR", "INCOME")
# The attribute "instance weight" should *not* be used in the
# classifiers, so it is set to "ignore" in this file
training <- subset(training, select=-c(MARSUPWT))
training <- data.modificationOfCountryOfBirth(training)
training <- subset(training, select=-c(GRINST,HHDFMX))
z <- as.factor(training[,39])
# Fit the model with all the training data
control_tree <- tree.control(nobs=dim(training[,1:38])[1],mindev = 0.0001) # entire tree
tr <- tree(z ~ ., data.frame(training[,1:38]), control = control_tree) # zapp has to be factor
validation <- cv.tree(tr, FUN = prune.misclass)
cv.tree(tr)
cvtree <- cv.tree(tr)
plot(cvtree)
best.size <- cvtree$size[which(cvtree$dev==min(cvtree$dev))] # which size is better?
best.size
best.size <- cvtree$size[which(cvtree$dev==min(cvtree$dev))] # which size is better?
best.size
cv.model.pruned <- prune.misclass(tr, best=best.size)
cvtree$dev
min(cvtree$dev)
best.size <- cvtree$size[which(cvtree$dev==min(cvtree$dev))][1]
best.size
cv.model.pruned <- prune.misclass(tr, best=best.size)
summary(cv.model.pruned)
partition.tree(cv.model.pruned)
tr <- cv.model.pruned
test <- read.csv("Data/census_income_test.csv", header=F)
colnames(test) <- c("AAGE","ACLSWKR","ADTIND","ADTOCC", "AHGA",
"AHRSPAY", "AHSCOL", "AMARITL", "AMJIND",
"AMJOCC", "ARACE", "AREORGN", "ASEX", "AUNMEM",
"AUNTYPE", "AWKSTAT", "CAPGAIN", "CAPLOSS", "DIVVAL",
"FILESTAT", "GRINREG", "GRINST", "HHDFMX",
"HHDREL", "MARSUPWT", "MIGMTR1", "MIGMTR3", "MIGMTR4",
"MIGSAME", "MIGSUN","NOEMP", "PARENT",
"PEFNTVTY","PEMNTVTY","PENATVTY","PRCITSHP",
"SEOTR", "VETQVA", "VETYN", "WKSWORK","YEAR", "INCOME")
test <- data.modificationOfCountryOfBirth(test)
ztest <- as.numeric(test[,"INCOME"])
test <- subset(test, select=-c(GRINST,HHDFMX,INCOME))
prob <- predict(tr, test) # classe un jeu de données au moyen de l'arbre
pred <- as.matrix(max.col(prob))
err <- sum(pred != ztest)/length(ztest)
# confusion matrix
mc <- matrix(0, nrow=2,ncol=2)
mc[1,1] <- sum(pred[which(ztest==1)] == ztest[which(ztest==1)])
mc[1,2] <- sum(pred[which(ztest==1)] != ztest[which(ztest==1)])
mc[2,1] <- sum(pred[which(ztest==2)] != ztest[which(ztest==2)])
mc[2,2] <- sum(pred[which(ztest==2)] == ztest[which(ztest==2)])
mc
err
summary(tr)
prob <- predict(tr,z)
z
prob <- predict(tr,training)
prob <- predict(tr, test) # classe un jeu de données au moyen de l'arbre
prob
pred
err <- sum(pred != as.numeric(z))/length(ztest)
pred <- as.matrix(max.col(prob))
err <- sum(pred != as.numeric(z))/length(ztest)
prob <- predict(tr, training) # classe un jeu de données au moyen de l'arbre
pred <- as.matrix(max.col(prob))
err <- sum(pred != as.numeric(z))/length(ztest)
err
err <- sum(pred != as.numeric(z))/length(z)
summary(tr)
prob <- predict(tr, test) # classe un jeu de données au moyen de l'arbre
pred <- as.matrix(max.col(prob))
err <- sum(pred != ztest)/length(ztest)
err
setwd("C:/Users/Bénédicte/Documents/Dataiku")
source("Scripts/data.R")
source("Scripts/separ1.R")
library(tree)
training <- read.csv("Data/census_income_learn.csv", header=F)
colnames(training) <- c("AAGE","ACLSWKR","ADTIND","ADTOCC", "AHGA",
"AHRSPAY", "AHSCOL", "AMARITL", "AMJIND",
"AMJOCC", "ARACE", "AREORGN", "ASEX", "AUNMEM",
"AUNTYPE", "AWKSTAT", "CAPGAIN", "CAPLOSS", "DIVVAL",
"FILESTAT", "GRINREG", "GRINST", "HHDFMX",
"HHDREL", "MARSUPWT", "MIGMTR1", "MIGMTR3", "MIGMTR4",
"MIGSAME", "MIGSUN","NOEMP", "PARENT",
"PEFNTVTY","PEMNTVTY","PENATVTY","PRCITSHP",
"SEOTR", "VETQVA", "VETYN", "WKSWORK","YEAR", "INCOME")
# The attribute "instance weight" should *not* be used in the
# classifiers, so it is set to "ignore" in this file
training <- subset(training, select=-c(MARSUPWT))
training <- data.modificationOfCountryOfBirth(training)
training <- subset(training, select=-c(GRINST,HHDFMX))
# 10-fold cross-validation
# Randomly shuffle the training data
training <- training[sample(nrow(training)),]
#Create 10 equally size folds
folds <- cut(seq(1,nrow(training)), breaks=10, labels=FALSE)
folds
err <- matrix(nrow=10, ncol=1)
#Perform 10-fold cross validation
for(i in 1:10){
print(i)
testIndexes <- which(folds==i,arr.ind = TRUE)
testData <- training[testIndexes, ]
trainData <- training[-testIndexes, ]
Xtest <- as.data.frame(testData)[,1:length(testData)-1]
ztest <- factor(testData[,length(testData)])
Xtrain <- as.data.frame(trainData)[,1:length(trainData)-1]
ztrain <- factor(trainData[,length(trainData)])
# Tree
control_tree <- tree.control(nobs=dim(Xtrain)[1],mindev = 0.0001) # entire tree
tr <- tree(ztrain ~ ., data.frame(Xtrain), control = control_tree) # zapp has to be factor
#validation <- cv.tree(tr, FUN = prune.misclass)
cvtree <- cv.tree(tr)
best.size <- cvtree$size[which(cvtree$dev==min(cvtree$dev))][1]
tr <- prune.misclass(tr, best=best.size)
prob <- predict(tr, Xtest)
pred <- as.matrix(max.col(prob))
err[i] <- sum(levels(ztest)[pred] != ztest)/length(ztest)
err[i]
}
IC <- function(moy,var,n,alpha) {
student <- qt(1-alpha/2, df = n-1)
IC <- c(moy - student*sqrt(var/n), moy + student*sqrt(var/n))
IC
}
err_mean <- mean(err)
var <- apply(err,1, function(x) ((x-err_mean)^2))
var <- 1/(10-1)*apply(matrix(var),2,sum)
IC <- IC(err_mean,var,10,0.05)
IC
err
err_mean
err <- matrix(nrow=10, ncol=1)
#Perform 10-fold cross validation
for(i in 1:10){
print(i)
testIndexes <- which(folds==i,arr.ind = TRUE)
testData <- training[testIndexes, ]
trainData <- training[-testIndexes, ]
Xtest <- as.data.frame(testData)[,1:length(testData)-1]
ztest <- factor(testData[,length(testData)])
Xtrain <- as.data.frame(trainData)[,1:length(trainData)-1]
ztrain <- factor(trainData[,length(trainData)])
# Tree
control_tree <- tree.control(nobs=dim(Xtrain)[1],mindev = 0.0001) # entire tree
tr <- tree(ztrain ~ ., data.frame(Xtrain), control = control_tree) # zapp has to be factor
validation <- cv.tree(tr, FUN = prune.misclass)
#cvtree <- cv.tree(tr)
#best.size <- cvtree$size[which(cvtree$dev==min(cvtree$dev))][1]
#tr <- prune.misclass(tr, best=best.size)
prob <- predict(tr, Xtest)
pred <- as.matrix(max.col(prob))
err[i] <- sum(levels(ztest)[pred] != ztest)/length(ztest)
print(err[i])
}
IC <- function(moy,var,n,alpha) {
student <- qt(1-alpha/2, df = n-1)
IC <- c(moy - student*sqrt(var/n), moy + student*sqrt(var/n))
IC
}
err_mean <- mean(err)
var <- apply(err,1, function(x) ((x-err_mean)^2))
var <- 1/(10-1)*apply(matrix(var),2,sum)
IC <- IC(err_mean,var,10,0.05)
err_mean
IC
err <- c(0.04891495,0.04956897,0.05022053,0.04726343,0.04801283,0.04761427,
0.04926824,0.04671211,0.05012029,0.04821330)
err_mean <- mean(err)
var <- apply(err,1, function(x) ((x-err_mean)^2))
var <- 1/(10-1)*apply(matrix(var),2,sum)
IC <- IC(err_mean,var,10,0.05)
IC <- function(moy,var,n,alpha) {
student <- qt(1-alpha/2, df = n-1)
IC <- c(moy - student*sqrt(var/n), moy + student*sqrt(var/n))
IC
}
err_mean <- mean(err)
var <- apply(err,1, function(x) ((x-err_mean)^2))
var <- 1/(10-1)*apply(matrix(var),2,sum)
IC <- IC(err_mean,var,10,0.05)
er
err
err <- as.matrix(err)
err_mean <- mean(err)
var <- apply(err,1, function(x) ((x-err_mean)^2))
var <- 1/(10-1)*apply(matrix(var),2,sum)
IC <- IC(err_mean,var,10,0.05)
IC <- function(moy,var,n,alpha) {
student <- qt(1-alpha/2, df = n-1)
IC <- c(moy - student*sqrt(var/n), moy + student*sqrt(var/n))
IC
}
IC <- IC(err_mean,var,10,0.05)
IC
